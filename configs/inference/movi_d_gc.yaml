experiment_group: icml26_ocl_rethinking
experiment_name: movi_d

globals:
  NUM_SLOTS: 15
  SLOT_DIM: 128
  DINO_MODEL: vit_base_patch14_dinov2
  FEAT_DIM: 768
  NUM_PATCHES: 576
  NUM_GPUS: 1
  BATCH_SIZE_PER_GPU: 8
  TOTAL_BATCH_SIZE: "${mul: ${.NUM_GPUS}, ${.BATCH_SIZE_PER_GPU}}"
  BASE_LR: 0.0001

trainer:
  max_steps: 100000
  log_every_n_steps: 250
  val_check_interval: 2500
  gradient_clip_val: 0.05

optimizer:
  name: Adam
  # Scale learning rate by batch size: take base lr once for every 32 samples
  lr: 0.0004
  lr_scheduler:
    name: exp_decay_with_warmup
    warmup_steps: 2500
    decay_steps: ${trainer.max_steps}

model:
  input_type: video
  visualize: false
  visualize_every_n_steps: 10000
  use_backbone_features: false  # Use stable DINO features for greedy init
  temporal_cross_window: 0  # 0=same-frame cycle, 0<=temporal cross-consistency<=chunk_size-1
  temporal_cross_mode: "both"  # or "backward" or "forward"
  losses:
    loss_featrec:
      name: MSELoss
      pred_dims:
        - 0
        - ${globals.FEAT_DIM}
    loss_ss:
      name: Slot_Slot_Contrastive_Loss
      pred_key: processor.state
      temperature: 0.1
      batch_contrast: true
      patch_inputs: false
      keep_input_dim: true
    loss_cycle:
      name: CycleConsistencyLoss
      pred_key: processor.cycle_slots
      target_key: processor.cycle_targets

  loss_weights:
    loss_featrec: 1.0
    loss_ss: 0.5
    loss_cycle: 0.0

  initializer:
    name: GreedyFeatureInit # FixedLearnedInit, GreedyFeatureInit, GreedyFeatureInitV2, ClusterFeatureInit
    n_slots: ${globals.NUM_SLOTS}
    dim: ${globals.SLOT_DIM}
    saliency_mode: local_consistency  # 'norm', 'entropy', 'variance', 'pca', 'local_consistency', 'local_consistency_pca', 'local_consistency_ms', 'local_consistency_uniform', 'local_consistency_centroid', 'local_consistency_balanced', 'local_consistency_soft', 'local_consistency_density', 'local_consistency_second', 'local_consistency_gaussian'
    neighbor_radius: 1
    selection_mode: hard # hard, soft, neighbor_avg, knn_refine, centroid
    refine_linear: false  # Enable learnable refinement (V1 only)
    refine_hidden: 0
    saliency_alpha: 0.5    # Try: 0.5, 0.7, 1.0, 1.3, 1.5
    spatial_suppression_radius: 0      # 1=3x3, 2=5x5, 3=7x7
    spatial_suppression_strength: 0.5  # 0=pure feature, 1=pure spatial
    soft_topk: 5
    temperature: 0.1 
    neighbor_avg_radius: 1
    saliency_smoothing: 0 # 0=no smoothing, 1=3x3, 2=5x5, 3=7x7, etc.
    init_mode: per_frame  # per_frame or first_frame
    aggregate: false           # Enable patch aggregation
    aggregate_threshold: 0.5  # Include patches with sim > 0.5
    # === V2-specific: Variable slot support ===
    init_threshold: 0.0        # Early stopping threshold (0=disabled, try 0.1-0.3)
    min_slots: 1               # Minimum slots before early stopping applies
    # === Clustering (ClusterFeatureInit only) ===
    cluster_method: agglomerative
    affinity: cosine
    linkage: average

  encoder:
    backbone:
      name: TimmExtractor
      model: ${globals.DINO_MODEL}
      features:
      - vit_block12
      - vit_block_keys12
      frozen: true
      pretrained: true
      model_kwargs:
        dynamic_img_size: True
    # 3D Fourier positional embedding (optional - requires depth data)
    use_pos_embed: false
    pos_embed:
      name: utils.LearnedPositionEmbed # CoordinatePositionEmbed RotaryPositionEmbed
      dim: ${globals.FEAT_DIM}
      n_patches: ${globals.NUM_PATCHES}
    output_transform:
      name: networks.two_layer_mlp
      inp_dim: ${globals.FEAT_DIM}
      outp_dim: ${globals.SLOT_DIM}
      hidden_dim: "${mul: ${globals.FEAT_DIM}, 2}"
      layer_norm: true

  grouper:
    name: SlotAttention
    inp_dim: ${globals.SLOT_DIM}
    slot_dim: ${globals.SLOT_DIM}
    n_iters: 2
    use_mlp: true
    use_gru: true
    use_ttt: false
    use_gated: false
    use_ent: false

  latent_processor:
    first_step_corrector_args:
      n_iters: 3
    use_ttt3r: false
    skip_corrector: false  # Bypass slot attention, use greedy-initialized slots directly
    skip_predictor: false  # Bypass predictor, use greedy-initialized slots directly

  decoder:
    name: MLPDecoder
    inp_dim: ${globals.SLOT_DIM}
    outp_dim: ${globals.FEAT_DIM}
    hidden_dims: [1024, 1024, 1024]
    n_patches: ${globals.NUM_PATCHES}

  predictor:
    name: networks.HungarianPredictor # networks.TransformerEncoder, networks.HungarianPredictor, networks.HungarianMemoryMatcher
    dim: ${globals.SLOT_DIM}
    n_blocks: 1
    n_heads: 4
    use_gated: false
    # === HungarianPredictor / HungarianMemoryMatcher options ===
    similarity: cosine         # 'cosine' or 'l2'
    pre_match: false           # Match BEFORE slot attention (HungarianPredictor only)
    # === HungarianMemoryMatcher V2 options (persistent tracking) ===
    max_slots: ${globals.NUM_SLOTS}  # Fixed registry capacity
    match_threshold: 2.0       # Cosine distance threshold for valid match (lower=stricter)
    ema_decay: 0.0             # EMA weight: new = decay*old + (1-decay)*current

val_metrics:
  ari:
    name: VideoARI
    ignore_background: true
    pred_key: decoder_masks_hard
    true_key: segmentations
  fg_ari:
    name: VideoARI
    ignore_background: true
    pred_key: decoder_masks_hard
    true_key: segmentations
  image_ari:
    name: ImageARI
    ignore_background: true
    video_input: true
    pred_key: decoder_masks_hard
    true_key: segmentations
  mbo:
    name: VideoIoU
    matching: overlap
    ignore_background: true
    pred_key: decoder_masks_hard
    true_key: segmentations
  image_mbo:
    name: ImageIoU
    matching: overlap
    ignore_background: true
    video_input: true
    pred_key: decoder_masks_hard
    true_key: segmentations

dataset:
  train_shards: "movi_d/movi_d-train-{000000..000304}.tar"
  val_shards: "movi_d/movi_d-validation-{000000..000007}.tar"
  batch_size: ${globals.BATCH_SIZE_PER_GPU}
  val_batch_size: 4
  val_size: 250
  num_workers: 4
  num_val_workers: 2
  train_pipeline:
    video_size: 24
    chunk_size: 4
    sample_one_chunk_per_video: true
    keys: [video]
    shuffle_size: 512
    transforms:
      name: movi_train
      type: video
      input_size: 336
      h_flip_prob: 0.5
  val_pipeline:
    use_chunks: false
    keys: [video, segmentations]
    transforms:
      name: movi_val
      type: video
      input_size: 336
      num_classes: 24